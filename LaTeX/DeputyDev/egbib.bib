@inproceedings{Vijayvergiya_2024, series={AIware ’24},
   title={AI-Assisted Assessment of Coding Practices in Modern Code Review},
   volume={24},
   url={http://dx.doi.org/10.1145/3664646.3665664},
   DOI={10.1145/3664646.3665664},
   booktitle={Proceedings of the 1st ACM International Conference on AI-Powered Software},
   publisher={ACM},
   author={Vijayvergiya, Manushree and Salawa, Małgorzata and Budiselić, Ivan and Zheng, Dan and Lamblin, Pascal and Ivanković, Marko and Carin, Juanjo and Lewko, Mateusz and Andonov, Jovan and Petrović, Goran and Tarlow, Daniel and Maniatis, Petros and Just, René},
   year={2024},
   month=jul, pages={85–93},
   collection={AIware ’24}
}

@inproceedings{10.1145/1357054.1357072,
author = {Mark, Gloria and Gudith, Daniela and Klocke, Ulrich},
title = {The cost of interrupted work: more speed and stress},
year = {2008},
isbn = {9781605580111},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1357054.1357072},
doi = {10.1145/1357054.1357072},
abstract = {We performed an empirical study to investigate whether the context of interruptions makes a difference. We found that context does not make a difference but surprisingly, people completed interrupted tasks in less time with no difference in quality. Our data suggests that people compensate for interruptions by working faster, but this comes at a price: experiencing more stress, higher frustration, time pressure and effort. Individual differences exist in the management of interruptions: personality measures of openness to experience and need for personal structure predict disruption costs of interruptions. We discuss implications for how system design can support interrupted work.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {107–110},
numpages = {4},
keywords = {context, experiment, interruptions, multi-tasking},
location = {Florence, Italy},
series = {CHI '08}
}

@misc{softwareCodeTime,
	author = {},
	title = {{C}ode {T}ime {R}eport | {S}oftware.com --- software.com},
	howpublished = {\url{https://www.software.com/reports/code-time-report}},
	year = {},
        key=2,
	note = {[Accessed 11-10-2024]},
}

@misc{tufano2024codereviewautomationstrengths,
      title={Code Review Automation: Strengths and Weaknesses of the State of the Art}, 
      author={Rosalia Tufano and Ozren Dabić and Antonio Mastropaolo and Matteo Ciniselli and Gabriele Bavota},
      year={2024},
      eprint={2401.05136},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2401.05136}, 
}

@misc{tufano2022usingpretrainedmodelsboost,
      title={Using Pre-Trained Models to Boost Code Review Automation}, 
      author={Rosalia Tufano and Simone Masiero and Antonio Mastropaolo and Luca Pascarella and Denys Poshyvanyk and Gabriele Bavota},
      year={2022},
      eprint={2201.06850},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2201.06850}, 
}

@inproceedings{10.1145/3540250.3549119,
author = {Hong, Yang and Tantithamthavorn, Chakkrit and Thongtanunam, Patanamon and Aleti, Aldeida},
title = {CommentFinder: a simpler, faster, more accurate code review comments recommendation},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3549119},
doi = {10.1145/3540250.3549119},
abstract = {Code review is an effective quality assurance practice, but can be labor-intensive since developers have to manually review the code and provide written feedback. Recently, a Deep Learning (DL)-based approach was introduced to automatically recommend code review comments based on changed methods. While the approach showed promising results, it requires expensive computational resource and time which limits its use in practice. To address this limitation, we propose CommentFinder – a retrieval-based approach to recommend code review comments. Through an empirical evaluation of 151,019 changed methods, we evaluate the effectiveness and efficiency of CommentFinder against the state-of-the-art approach. We find that when recommending the best-1 review comment candidate, our CommentFinder is 32\% better than prior work in recommending the correct code review comment. In addition, CommentFinder is 49 times faster than the prior work. These findings highlight that our CommentFinder could help reviewers to reduce the manual efforts by recommending code review comments, while requiring less computational time.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {507–519},
numpages = {13},
keywords = {Software Quality Assurance, Modern Code Review},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@misc{deeplearningFourAgent,
	author = {Andrew Ng},
	title = {{F}our {A}{I} {A}gent {S}trategies {T}hat {I}mprove {G}{P}{T}-4 and {G}{P}{T}-3.5 {P}erformance --- deeplearning.ai},
	howpublished = {\url{https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance}},
	year = {2024},
	note = {[Accessed 11-10-2024]},
}

@misc{deeplearningAgenticDesign,
	author = {Andrew Ng},
	title = {{A}gentic {D}esign {P}atterns {P}art 5, {M}ulti-{A}gent {C}ollaboration --- deeplearning.ai},
	howpublished = {\url{https://www.deeplearning.ai/the-batch/agentic-design-patterns-part\\-5-multi-agent-collaboration/}},
	year = {2024},
	note = {[Accessed 11-10-2024]},
}

@misc{Introduc39:online,
author = {Michelle Pokrass},
title = {Introducing Structured Outputs in the API | OpenAI},
howpublished = {\url{https://openai.com/index/introducing-structured-outputs-in-the-api/}},
month = {Auguest},
year = {2024},
note = {(Accessed on 10/11/2024)}
}

@misc{tam2024letspeakfreelystudy,
      title={Let Me Speak Freely? A Study on the Impact of Format Restrictions on Performance of Large Language Models}, 
      author={Zhi Rui Tam and Cheng-Kuang Wu and Yi-Lin Tsai and Chieh-Yen Lin and Hung-yi Lee and Yun-Nung Chen},
      year={2024},
      eprint={2408.02442},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.02442}, 
}

@misc{anthropic,
	author = {Anthropic},
	title = {www-cdn.anthropic.com},
	howpublished = {\url{https://www-cdn.anthropic.com/fed9cc193a14b84131812372d8d5857f8f304c52/Model_Card_Claude_3_Addendum.pdf}},
	year = {2024},
	note = {[Accessed 11-10-2024]},
}


@misc{qian2024chatdevcommunicativeagentssoftware,
      title={ChatDev: Communicative Agents for Software Development}, 
      author={Chen Qian and Wei Liu and Hongzhang Liu and Nuo Chen and Yufan Dang and Jiahao Li and Cheng Yang and Weize Chen and Yusheng Su and Xin Cong and Juyuan Xu and Dahai Li and Zhiyuan Liu and Maosong Sun},
      year={2024},
      eprint={2307.07924},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2307.07924}, 
}

@misc{shinn2023reflexionlanguageagentsverbal,
      title={Reflexion: Language Agents with Verbal Reinforcement Learning}, 
      author={Noah Shinn and Federico Cassano and Edward Berman and Ashwin Gopinath and Karthik Narasimhan and Shunyu Yao},
      year={2023},
      eprint={2303.11366},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2303.11366}, 
}

@misc{madaan2023selfrefineiterativerefinementselffeedback,
      title={Self-Refine: Iterative Refinement with Self-Feedback}, 
      author={Aman Madaan and Niket Tandon and Prakhar Gupta and Skyler Hallinan and Luyu Gao and Sarah Wiegreffe and Uri Alon and Nouha Dziri and Shrimai Prabhumoye and Yiming Yang and Shashank Gupta and Bodhisattwa Prasad Majumder and Katherine Hermann and Sean Welleck and Amir Yazdanbakhsh and Peter Clark},
      year={2023},
      eprint={2303.17651},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.17651}, 
}



