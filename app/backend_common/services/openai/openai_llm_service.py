from typing import List

from app.backend_common.service_clients.openai.openai import OpenAIServiceClient
from app.backend_common.services.openai.openai_service import OpenAIManager


class OpenAILLMService:
    """
    Client class for interacting with the Language Model (LLM) service.
    """

    def __init__(self, client_type: str = "openai") -> None:
        self.client_type = client_type

    async def get_embeddings(self, input_data: List[str], store_embeddings: bool = True) -> List[List[float]]:
        """
        Get embeddings from the Language Model service.

        Args:
            input_data (LLMClientInput): Input data containing batch of text.
            store_embeddings (bool): If true we will store embeddings in Redis

        Returns:
            LLMClientOutput: Embeddings generated by the Language Model.
        """
        if self.client_type != "openai":
            raise ValueError("Unsupported client type. Only OpenAI is supported as of now.")

        try:
            embeddings, input_tokens = await OpenAIManager().get_embeddings(input_data, store_embeddings)
        except Exception as e:
            raise ValueError(f"Failed to get embeddings: {e}")

        return embeddings, input_tokens

    async def get_client_response(self, conversation_message: list, model: str, response_type: str = "json_object"):
        """
        Makes a call to OpenAI chat completion API with a specific model and conversation messages.
        Implements a retry mechanism in case of a failed request or improper response.

        Args:
            conversation_message: System and user message object.
            model: GPT model to be called.
            confidence_filter_score: Score to filter the comments.
            max_retry: Number of times OpenAI should be called in case of failure or improper response.

        Returns:
           List of filtered comment objects.
        """
        if self.client_type != "openai":
            raise ValueError("Unsupported client type. Only OpenAI is supported as of now.")

        response = await OpenAIServiceClient().get_llm_non_stream_response_chat_api(
            conversation_messages=conversation_message, model=model, response_type=response_type
        )
        return response.choices[0].message, response.usage
